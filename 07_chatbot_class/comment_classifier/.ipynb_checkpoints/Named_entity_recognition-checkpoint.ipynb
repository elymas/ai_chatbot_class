{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER with Bidirectional-LSTM-CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original paper can be found at https://arxiv.org/abs/1511.08308"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # Colab only\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.16.2\n",
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab에서 실행 중이라면...\n",
    "!git clone https://github.com/hukim1112/comment_classifier.git\n",
    "import os\n",
    "os.chdir('/content/comment_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from konlpy.tag import Twitter\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from matplotlib import pyplot as plt\n",
    "from data_utils import createBatches, iterate_minibatches\n",
    "import ner\n",
    "keras = tf.keras\n",
    "t = Twitter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. read train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'train_entity.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, encoding='euc-kr') as f:\n",
    "    tokenized_sentences, labels = [], []\n",
    "    tokenized_sentence, label = [], []\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if (len(line) == 0 or line.startswith(\"-DOCSTART-\")):\n",
    "            if not len(tokenized_sentence) == 0:\n",
    "                tokenized_sentences.append(tokenized_sentence)\n",
    "                labels.append(label)\n",
    "                tokenized_sentence, label = [], [] #초기화\n",
    "        else:\n",
    "            word, tag = line.split(' ')\n",
    "            tokenized_sentence.append(word)\n",
    "            label.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get vectorizer and fix some data error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorizer import BaseVectorizer\n",
    "vectorizer = BaseVectorizer(t.morphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tokenized sentence : \", tokenized_sentences[1606], '\\n',\n",
    "      \"labels : \", labels[1606])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.tokenizer('말해줄래') #우리 tokenizer와 다른 형태소 분석 형태로 데이터가 구성되어 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ts, label in zip(tokenized_sentences, labels):\n",
    "    for idx, word in enumerate(ts):\n",
    "        if len(vectorizer.tokenizer(word))>1:\n",
    "            tokenized_word = vectorizer.tokenizer(word)\n",
    "            ts.pop(idx)\n",
    "            tag = label.pop(idx)\n",
    "            for i in tokenized_word[::-1]:\n",
    "                ts.insert(idx, i)\n",
    "                label.insert(idx, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences[1606]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[1606]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create word vocabulary and char vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for ts in tokenized_sentences:\n",
    "    sentence = ' '.join(ts)\n",
    "    sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(sentences) #create word vocabulary from docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_char2idx() \n",
    "#create dictionary for converting char into index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2idx = {}\n",
    "idx2label = []\n",
    "for label in labels:\n",
    "    for l in label:\n",
    "        if l not in label2idx:\n",
    "            label2idx[l] = len(label2idx)\n",
    "            idx2label.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label2idx, idx2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Prepare dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 15\n",
    "\n",
    "def padding_char_indice(char_indice, MAX_LENGTH):\n",
    "    return tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      char_indice, maxlen=MAX_LENGTH, padding='post', \n",
    "      value = vectorizer.char2idx['_PAD_'])\n",
    "    \n",
    "\n",
    "def integer_coding(tokenized_sentences, labels):\n",
    "    dataset = []\n",
    "    for ts, label in zip(tokenized_sentences, labels):\n",
    "        word_indice = [vectorizer.vocabulary_[t] for t in ts]\n",
    "        char_indice = [[vectorizer.char2idx[char] for char in t]  \n",
    "                                                     for t in ts]\n",
    "        char_indice = padding_char_indice(char_indice, MAX_LENGTH)\n",
    "        label_indice = [label2idx[l] for l in label]\n",
    "    \n",
    "        for chars_of_token in char_indice:\n",
    "            if len(chars_of_token)>MAX_LENGTH:\n",
    "                print(\"최대 단어 길이 초과!\")\n",
    "                continue\n",
    "        dataset.append([word_indice, char_indice, label_indice])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = integer_coding(tokenized_sentences, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indice, char_indice, label_indice = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.decode_from_list(word_indice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[idx2label[l] for l in label_indice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_indice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "np.save(\"models/idx2Label.npy\",idx2label)\n",
    "np.save(\"models/word2Idx.npy\",vectorizer.vocabulary_)\n",
    "np.save(\"models/char2Idx.npy\",vectorizer.char2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch,train_batch_len = createBatches(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,batch in enumerate(iterate_minibatches(train_batch,train_batch_len)):\n",
    "        labels, tokens ,char = batch\n",
    "        print(tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, TimeDistributed, Dropout, concatenate, Bidirectional, LSTM, Conv1D, Dense, MaxPooling1D, Flatten\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras.utils import Progbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_input = Input(shape=(None,),dtype='int32',name='words_input')\n",
    "words = Embedding(input_dim=len(vectorizer.vocabulary_), output_dim=64)(words_input)\n",
    "character_input=Input(shape=(None,MAX_LENGTH,),name='char_input')\n",
    "embed_char_out=TimeDistributed(Embedding(len(vectorizer.char2idx),30,embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(character_input)\n",
    "dropout= Dropout(0.5)(embed_char_out)\n",
    "conv1d_out= TimeDistributed(Conv1D(kernel_size=3, filters=30, padding='same',activation='tanh', strides=1))(dropout)\n",
    "maxpool_out=TimeDistributed(MaxPooling1D(MAX_LENGTH))(conv1d_out)\n",
    "char = TimeDistributed(Flatten())(maxpool_out)\n",
    "char = Dropout(0.5)(char)\n",
    "output = concatenate([words, char])\n",
    "output = Bidirectional(LSTM(200, return_sequences=True, dropout=0.50, recurrent_dropout=0.25))(output)\n",
    "output = TimeDistributed(Dense(len(label2idx), activation='softmax'))(output)\n",
    "model = Model(inputs=[words_input,character_input], outputs=[output])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(model, 'model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):    \n",
    "    print(\"Epoch %d/%d\"%(epoch,epochs))\n",
    "    a = Progbar(len(train_batch_len))\n",
    "    for i,batch in enumerate(iterate_minibatches(train_batch,train_batch_len)):\n",
    "        labels, tokens, char = batch       \n",
    "        model.train_on_batch([tokens, char], labels)\n",
    "        a.update(i)\n",
    "    a.update(i+1)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_parser = ner.Parser(t.morphs)\n",
    "ner_parser.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_parser.predict('내일 부산 날씨는?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_parser.predict('오늘 서울 날씨 어때?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_parser.predict('9월 15일 수유동 날씨 궁금해')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
