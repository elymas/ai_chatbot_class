{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/masterp/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(44506, 3)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import pandas as pd\n",
    "import io, requests\n",
    "\n",
    "movies = pd.read_csv(\"movies_metadata.csv\", usecols=['original_title', 'overview', 'title'])\n",
    "movies = movies.dropna(axis=0)\n",
    "movies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_title</th>\n",
       "      <th>overview</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>Toy Story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>Jumanji</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>A family wedding reignites the ancient feud be...</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     original_title                                           overview  \\\n",
       "0         Toy Story  Led by Woody, Andy's toys live happily in his ...   \n",
       "1           Jumanji  When siblings Judy and Peter discover an encha...   \n",
       "2  Grumpier Old Men  A family wedding reignites the ancient feud be...   \n",
       "\n",
       "              title  \n",
       "0         Toy Story  \n",
       "1           Jumanji  \n",
       "2  Grumpier Old Men  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_plot_li = movies['overview']\n",
    "movie_info_li = movies['title']\n",
    "movies.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.tokenizer = RegexpTokenizer('(?u)[A-z]+')\n",
    "    def __call__(self, doc):    #클래스 호출시 마다 실행(Tf-idf Vector 호출)\n",
    "        return([self.wnl.lemmatize(t) for t in self.tokenizer.tokenize(doc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "#사이킷런에 위에서 정의한 토크나이저를 입력으로 넣습니다.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=3, tokenizer=LemmaTokenizer(), stop_words='english')\n",
    "X = vectorizer.fit_transform(movie_plot_li[:10000])\n",
    "vocabluary = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.02930725, 0.00580921, ..., 0.01088994, 0.01071711,\n",
       "        0.        ],\n",
       "       [0.02930725, 1.        , 0.05286611, ..., 0.00904565, 0.00890209,\n",
       "        0.        ],\n",
       "       [0.00580921, 0.05286611, 1.        , ..., 0.00466609, 0.00459204,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.01088994, 0.00904565, 0.00466609, ..., 1.        , 0.00860824,\n",
       "        0.        ],\n",
       "       [0.01071711, 0.00890209, 0.00459204, ..., 0.00860824, 1.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#비슷한 영화 추천하는 Cosin 유사모델 만들기\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "movie_sim = cosine_similarity(X)\n",
    "print(movie_sim.shape)\n",
    "movie_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#특정 영화와 유사한 영화목록 출력하기\n",
    "\n",
    "def similar_recommend_by_movie_id(movielens_id, rank=8):\n",
    "    movie_index = movielens_id -1\n",
    "    similar_movies = sorted(list(enumerate(movie_sim[movie_index])), key=lambda x:x[1], reverse=True)\n",
    "    \n",
    "    print(\"----- {} : 관람객 추천영화 -----\".format(movie_info_li[similar_movies[0][0]]))\n",
    "    \n",
    "    for no, movie_idx in enumerate(similar_movies[1:rank]):\n",
    "        print('추천영화 {}순위 : {}'.format(no, movie_info_li[movie_idx[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Tom and Huck : 관람객 추천영화 -----\n",
      "추천영화 0순위 : Son in Law\n",
      "추천영화 1순위 : Heavy Metal\n",
      "추천영화 2순위 : Cecil B. Demented\n",
      "추천영화 3순위 : The Great Ziegfeld\n",
      "추천영화 4순위 : A Guy Thing\n",
      "추천영화 5순위 : Regeneration\n",
      "추천영화 6순위 : The Year of Living Dangerously\n",
      "추천영화 7순위 : The Black Scorpion\n",
      "추천영화 8순위 : DragonHeart\n",
      "추천영화 9순위 : Frankie and Johnny\n",
      "추천영화 10순위 : Lethal Weapon 2\n",
      "추천영화 11순위 : Bagdad Cafe\n",
      "추천영화 12순위 : Audition\n",
      "추천영화 13순위 : Rhyme & Reason\n"
     ]
    }
   ],
   "source": [
    "similar_recommend_by_movie_id(8, rank=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
